

# Big model Adversarial Attack
----
1.[White-box-Attack](#jump1)   

2.[Black-box-Attack](#jump2)  

---
## <span id="jump1">White-box-Attack</span>


### CVPR 2023

+ Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization  
Jianping Zhang, Yizhan Huang, Weibin Wu, Michael R. Lyu  
[[paper](https://arxiv.org/pdf/2303.15754.pdf)] [[code]()]  



+ GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis  
Ming Tao, Bing-Kun Bao, Hao Tang, Changsheng Xu  
[[paper](http://arxiv.org/abs/2301.12959)] [[code]()]  


+ Learning With Noisy Labels via Self-Supervised Adversarial Noisy Masking  
Yuanpeng Tu, Boshen Zhang, Yuxi Li, Liang Liu, Jian Li, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Cai Rong Zhao  
[[paper](http://arxiv.org/abs/2302.06805)] [[code]()]  


+ RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation With Natural Prompts  
Han Liu, Yuhao Wu, Shixuan Zhai, Bo Yuan, Ning Zhang
[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.pdf)] [[code]()]  



+ Exploring the Relationship Between Architectural Design and Adversarially Robust Generalization  
Aishan Liu, Shiyu Tang, Siyuan Liang, Ruihao Gong, Boxi Wu, Xianglong Liu, Dacheng Tao  
[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Exploring_the_Relationship_Between_Architectural_Design_and_Adversarially_Robust_Generalization_CVPR_2023_paper.pdf)] [[code]()]  



+ Semi-Supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination  
Zimeng Zhao, Binghui Zuo, Zhiyu Long, Yangang Wang  
[[paper](http://arxiv.org/abs/2303.06380)] [[code]()]  


+ Black-Box Sparse Adversarial Attack via Multi-Objective Optimisation  
Phoenix Neale Williams, Ke Li  
[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Williams_Black-Box_Sparse_Adversarial_Attack_via_Multi-Objective_Optimisation_CVPR_2023_paper.pdf)] [[code]()]  



+ CLIP2Protect: Protecting Facial Privacy Using Text-Guided Makeup via Adversarial Latent Search  
Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar  
[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Shamshad_CLIP2Protect_Protecting_Facial_Privacy_Using_Text-Guided_Makeup_via_Adversarial_Latent_CVPR_2023_paper.pdf)] [[code]()]  


+ CLIP2Protect: Protecting Facial Privacy Using Text-Guided Makeup via Adversarial Latent Search  
Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar  
[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Shamshad_CLIP2Protect_Protecting_Facial_Privacy_Using_Text-Guided_Makeup_via_Adversarial_Latent_CVPR_2023_paper.pdf)] [[code]()]  


+ TrojDiff: Trojan Attacks on Diffusion Models With Diverse Targets  
Weixin Chen, Dawn Song, Bo Li  
[[paper](http://arxiv.org/abs/2303.05762)] [[code]()]  


+ You Are Catching My Attention: Are Vision Transformers Bad Learners Under Backdoor Attacks?  
Zenghui Yuan, Pan Zhou, Kai Zou, Yu Cheng  
[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Yuan_You_Are_Catching_My_Attention_Are_Vision_Transformers_Bad_Learners_CVPR_2023_paper.pdf)] [[code]()]  


### arxiv 2022

+ On the Adversarial Robustness of Vision Transformers   
Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh  
[[paper](https://arxiv.org/abs/2103.15670)]  [[code](https://github.com/RulinShao/on-the-adversarial-robustness-of-visual-transformer)]  


### ICCV 2021

+ Understanding robustness of transformers for image classiÔ¨Åcation  
Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, Andreas Veit  
[[paper](https://arxiv.org/abs/2103.14586)] [[code]()]  
  <details>
    <summary>Notes</summary>
    test
    </details>





## <span id="jump2">Black-box-Attack</span>


### arxiv 2023

+ Defense-Prefix for Preventing Typographic Attacks on CLIP     
Hiroki Azuma, Yusuke Matsui    
[[paper](https://arxiv.org/pdf/2304.04512.pdf)]  [[code](https://github.com/RulinShao/on-the-adversarial-robustness-of-visual-transformer)]  





### arxiv 2022

+ Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal  
Yucheng Shi, Yahong Han, Yu-an Tan, Xiaohui Kuang  
[[paper](https://arxiv.org/abs/2112.03492)] [[code](https://github.com/shiyuchengTJU/PAR/blob/main/par_main.py)]  


+ Towards Transferable Adversarial Attacks on Vision Transformers  
Zhipeng Wei, Jingjing Chen, Micah Goldblum, Zuxuan Wu, Tom Goldstein, Yu-Gang Jiang  
[[paper](https://arxiv.org/abs/2109.04176)] [[code](https://github.com/shiyuchengTJU/PAR/blob/main/par_main.py)]  


+ On Improving Adversarial Transferability of Vision Transformers  
Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, Fatih Porikli  
[[paper](https://arxiv.org/abs/2106.04169)] [[code](https://t.ly/hBbW)]  


### CVPR 2022

+ A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes  
Mazda Moayeri, Phillip Pope, Yogesh Balaji  
[[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Moayeri_A_Comprehensive_Study_of_Image_Classification_Model_Sensitivity_to_Foregrounds_CVPR_2022_paper.pdf)] [[code]()]  
  <details>
    <summary>Notes</summary>
    In our analysis, we consider diverse state-of-the-art architectures (ResNets, Transformers) and training procedures (CLIP, SimCLR, DeiT, Adversarial Training). We find that, somewhat surprisingly, in ResNets, adversarial training makes models more sensitive to the background compared to foreground than standard training. Similarly, contrastively-trained models also have lower relative foreground sensitivity in both transformers and ResNets. Lastly, we observe intriguing adaptive abilities of transformers to increase relative foreground sensitivity as corruption level increases. Using saliency methods, we automatically discover spurious features that drive the background sensitivity of models and assess alignment of saliency maps with foregrounds
    </details>


