

# Big model Adversarial Attack
----
1.[White-box-Attack](#jump1)   

2.[Black-box-Attack](#jump2)  

---
## <span id="jump1">White-box-Attack</span>


### arxiv 2022

+ On the Adversarial Robustness of Vision Transformers   
Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh  
[[paper](https://arxiv.org/abs/2103.15670)]  [[code](https://github.com/RulinShao/on-the-adversarial-robustness-of-visual-transformer)]


### ICCV 2021

+ Understanding robustness of transformers for image classiÔ¨Åcation  
Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, Andreas Veit
[[paper](https://arxiv.org/abs/2103.14586)] [[code]()]
  <details>
    <summary>Notes</summary>
    test
    </details>





## <span id="jump2">Black-box-Attack</span>


### arxiv 2022

+ Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal
Yucheng Shi, Yahong Han, Yu-an Tan, Xiaohui Kuang  
[[paper](https://arxiv.org/abs/2112.03492)] [[code](https://github.com/shiyuchengTJU/PAR/blob/main/par_main.py)]  


+ Towards Transferable Adversarial Attacks on Vision Transformers  
Zhipeng Wei, Jingjing Chen, Micah Goldblum, Zuxuan Wu, Tom Goldstein, Yu-Gang Jiang  
[[paper](https://arxiv.org/abs/2109.04176)] [[code](https://github.com/shiyuchengTJU/PAR/blob/main/par_main.py)]  


+ On Improving Adversarial Transferability of Vision Transformers  
Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, Fatih Porikli  
[[paper](https://arxiv.org/abs/2106.04169)] [[code](https://t.ly/hBbW)]  



